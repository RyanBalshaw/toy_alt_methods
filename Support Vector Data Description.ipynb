{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b183844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cvxopt\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84224d2",
   "metadata": {},
   "source": [
    "## Convex optimisation problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c03324",
   "metadata": {},
   "source": [
    "The general convex optimization problem is:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\text{minimize } & \\frac{1}{2} \\mathbf{x}^T \\mathbf{P} \\mathbf{x} + \\mathbf{q}^T\\mathbf{x} \\\\\n",
    "\\text{subject to } & \\mathbf{G}\\mathbf{x} \\leq \\mathbf{h} \\\\\n",
    "& \\mathbf{A}\\mathbf{x} = \\mathbf{b}.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "In the general (soft or hard margin) SVDD formulation, the equations given in Tax, 2004 are as follows:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\max_\\mathbf{a} \\text{ } & \\mathcal{L}(\\mathbf{a}) = \\sum_{n=1}^{N}a_n k(\\mathbf{x}_n, \\mathbf{x}_n) -\\sum_{n=1}^{N}\\sum_{m=1}^N a_n a_m k(\\mathbf{x}_n, \\mathbf{x}_m), \\\\\n",
    "&\\text{such that } 0 \\leq a_n \\leq C \\\\\n",
    "& \\text{and } \\sum_{n=1}^N a_i = 1  \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "There is also a formulation for SVDD that includes negative samples. The derivation is slightly more involved, but at the end of the derivation it is revealed that if one defines a variable $a^{'}_i$ as $a^{'}_i = y_i a_i$, we can simply reformulate the problem as \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\max_\\mathbf{a}^{'} \\text{ } & \\mathcal{L}(\\mathbf{a}^{'}) = \\sum_{n=1}^{N}a^{'}_n k(\\mathbf{x}_n, \\mathbf{x}_n) -\\sum_{n=1}^{N}\\sum_{m=1}^N a^{'}_n a^{'}_m k(\\mathbf{x}_n, \\mathbf{x}_m), \\\\\n",
    "&\\text{such that } 0 \\leq a^{'}_n \\leq C \\\\\n",
    "& \\text{and } \\sum_{n=1}^N a^{'}_i = 1  \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Note that the variable $y_i \\in \\{-1, 1\\}$, and $y_i=-1$ is given to samples that the radius should not encompass. Finally, we can calculate the centroid $\\alpha$ using\n",
    "\\begin{equation}\n",
    "\\boldsymbol\\alpha = \\sum a^{'}_i \\mathbf{x_i}.\n",
    "\\end{equation}\n",
    "\n",
    "If we test new data, we can do so using the radius formula\n",
    "\\begin{equation}\n",
    "R^2 = k(\\mathbf{x}_k, \\mathbf{x}_k) - 2 \\sum_i a^{'}_i k(\\mathbf{x}_i, \\mathbf{x}_k) + \\sum_{i}\\sum_{j} k(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{x}_k$ is any support vector. This radius is used to test new data using:\n",
    "\\begin{equation}\n",
    "\\Vert \\mathbf{z} - \\boldsymbol\\alpha \\Vert_2^2 = k(\\mathbf{z}, \\mathbf{z}) - 2 \\sum_i a^{'}_i k(\\mathbf{z}, \\mathbf{x}_k) + \\sum_{i}\\sum_{j} k(\\mathbf{x}_i, \\mathbf{x}_j) \\leq R^2\n",
    "\\end{equation}\n",
    "\n",
    "we can subtract $R^2 - \\Vert \\mathbf{z} - \\boldsymbol\\alpha \\Vert_2^2$ to produce a positive value if a test sample is in the decision boundary, or a negative value otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfedef8",
   "metadata": {},
   "source": [
    "We can immediately see that we can rewrite this to be suitable to the convex optimisation problem: let us reformulate the convex problem with $\\mathbf{x} = \\mathbf{a}$ to give the following matrices (let $\\otimes$ be the outer product)\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P} = 2 * \\mathbf{K}(X),\n",
    "\\end{equation}\n",
    "where $\\mathbf{K}$ is the gram matrix:\n",
    "\\begin{equation}\n",
    "K_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j),\n",
    "\\end{equation}\n",
    "\n",
    "Furthermore, $\\mathbf{q}$ is simply:\n",
    "\\begin{equation}\n",
    "\\mathbf{q} = -\\text{diag}(\\mathbf{K}).\n",
    "\\end{equation}\n",
    "\n",
    "If we solve to hard margin problem ($C = \\infty$, $a_n \\geq 0$), then $\\mathbf{G}$ is simply the negative identify matrix.\n",
    "\\begin{equation}\n",
    "\\mathbf{G}_{hard} = - \\mathbf{I}\n",
    "\\end{equation}\n",
    "and $\\mathbf{h}$ is simply a zero vector\n",
    "\\begin{equation}\n",
    "\\mathbf{h} = \\mathbf{0}.\n",
    "\\end{equation}\n",
    "\n",
    "However, if we solve to soft margin problem ($0\\leq a_n \\leq C$), $\\mathbf{G}$ and $\\mathbf{h}$ become\n",
    "\\begin{equation}\n",
    "\\mathbf{G}_{soft} = [\\mathbf{G_1}, \\mathbf{G_2}]^T\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\mathbf{G_1} = -\\mathbf{I}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\mathbf{G_2} = \\mathbf{I}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\mathbf{h} = [\\mathbf{h}_1, \\mathbf{h}_2]^T\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\mathbf{h}_1 = \\mathbf{0}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\mathbf{h}_2 = C * \\mathbf{1}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, the final equality constraint is simple, as both $\\mathbf{A} = \\mathbf{1}$, where $\\mathbf{A}$ is the one vector and $\\mathbf{b} = 1$.\n",
    "\n",
    "We can also demonstrate how the convex problem is formulated for $a^{'}_i = y_ia_i$: let us reformulate the convex problem with $\\mathbf{x} = \\mathbf{a}$ to give the following matrices (let $\\otimes$ be the outer product):\n",
    "\\begin{equation}\n",
    "\\mathbf{P} = 2 * \\mathbf{t}\\otimes \\mathbf{t} * \\mathbf{K}(X),\n",
    "\\end{equation}\n",
    "where $\\mathbf{K}$ is the gram matrix:\n",
    "\\begin{equation}\n",
    "K_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j),\n",
    "\\end{equation}\n",
    "\n",
    "Furthermore, $\\mathbf{q}$ is simply:\n",
    "\\begin{equation}\n",
    "\\mathbf{q} = -\\mathbf{t} * \\text{diag}(\\mathbf{K}).T.\n",
    "\\end{equation}\n",
    "\n",
    "If we solve to hard margin problem ($C = \\infty$, $a_n \\geq 0$), then $\\mathbf{G}$ is simply the negative diagonal matrix with $\\mathbf{t}$ on the diagonal.\n",
    "\\begin{equation}\n",
    "\\mathbf{G}_{hard} = - \\text{diag}(\\mathbf{t})\n",
    "\\end{equation}\n",
    "and $\\mathbf{h}$ is simply a zero vector\n",
    "\\begin{equation}\n",
    "\\mathbf{h} = \\mathbf{0}.\n",
    "\\end{equation}\n",
    "\n",
    "However, if we solve to soft margin problem ($0\\leq a_n \\leq C$), $\\mathbf{G}$ and $\\mathbf{h}$ become\n",
    "\\begin{equation}\n",
    "\\mathbf{G}_{soft} = [\\mathbf{G_1}, \\mathbf{G_2}]^T\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\mathbf{G_1} = -\\text{diag}(\\mathbf{t})\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\mathbf{G_2} = \\text{diag}(\\mathbf{t})\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\mathbf{h} = [\\mathbf{h}_1, \\mathbf{h}_2]^T\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\mathbf{h}_1 = \\mathbf{0}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\mathbf{h}_2 = C * \\mathbf{1}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, the final equality constraint is simple, as both $\\mathbf{A} = \\mathbf{1}$, where $\\mathbf{A}$ is the one vector and $\\mathbf{b} = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e12df2",
   "metadata": {},
   "source": [
    "## SVDD Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b63c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDD(object):\n",
    "    def __init__(self, kernel = 'linear', degree = 3, sigma = 1, C = None, alpha_threshold = 1e-5):\n",
    "        \n",
    "        self.kernel = kernel\n",
    "        self.degree = degree #polynomial kernel degree\n",
    "        self.sigma = sigma #Gaussian kernel 1/variance\n",
    "        self.C = C #1/N <= C <= 1\n",
    "        self.alpha_threshold = alpha_threshold\n",
    "        self.radius_threshold = 1e-4\n",
    "\n",
    "    def linear_kernel(self, x1, x2):\n",
    "        return np.dot(x1, x2)\n",
    "    \n",
    "    def polynomial_kernel(self, x1, x2):\n",
    "        return (1 + np.dot(x1, x2)) **self.degree\n",
    "    \n",
    "    def gaussian_kernel(self, x1, x2):\n",
    "        return np.exp(-1 * (np.linalg.norm(x1 - x2)**2) / self.sigma**2 )\n",
    "    \n",
    "    def apply_kernel_function(self, x1, x2):\n",
    "        \n",
    "        if self.kernel == 'linear':\n",
    "            return self.linear_kernel(x1, x2)\n",
    "        \n",
    "        elif self.kernel == 'polynomial':\n",
    "            return self.polynomial_kernel(x1, x2)\n",
    "            \n",
    "        elif self.kernel == 'gaussian':\n",
    "            return self.gaussian_kernel(x1, x2)\n",
    "        \n",
    "        else:\n",
    "            print(\"Illegal kernel function chosen.\")\n",
    "            raise SystemExit\n",
    "    \n",
    "    def calculate_gram(self, X):\n",
    "        n, f = X.shape\n",
    "        \n",
    "        K = np.zeros((n, n))\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                K[i, j] = self.apply_kernel_function(X[i, :], X[j, :])\n",
    "        \n",
    "        return K\n",
    "    \n",
    "    def calculate_max_dist(self, X):\n",
    "        n, f = X.shape\n",
    "        \n",
    "        dist_mat = np.zeros((n, n))\n",
    "        \n",
    "        for i in range(n):\n",
    "            dist_mat[i, :] = np.sqrt(np.sum((X[i, :] - X)**2, axis = 1))\n",
    "            #for j in range(n):\n",
    "                #dist_mat[i, j] = np.linalg.norm(X[i, :] - X[j, :])**2\n",
    "        \n",
    "        return np.max(np.sqrt(dist_mat))\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        \n",
    "        #labels can be None, but \n",
    "        n = X.shape[0]\n",
    "        \n",
    "        if y is None:\n",
    "            y = np.ones(n)\n",
    "            \n",
    "        else:\n",
    "            assert len(y) == n, \"The labels are not the same shape as the input data X.\"\n",
    "        \n",
    "        #Determine gram matrix\n",
    "        Gram = self.calculate_gram(X)\n",
    "        \n",
    "        #Setup necessary matrices to be used for lagrangian multipliers\n",
    "        #P and q\n",
    "        \n",
    "        P = cvxopt.matrix(2 * np.outer(y, y) *  Gram)\n",
    "        q = cvxopt.matrix(-1 * y.reshape(-1, 1) * np.diag(Gram).reshape(-1, 1))\n",
    "        \n",
    "        #G and h\n",
    "        if self.C is None:\n",
    "            print(\"\\nSolving the hard margin problem!\")\n",
    "            G = cvxopt.matrix(-1 * np.diag(y))\n",
    "            h = cvxopt.matrix(np.zeros((n, 1)))\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nSolving the soft margin problem!\")\n",
    "            G_1 = -1 * np.diag(y)\n",
    "            h_1 = np.zeros((n, 1))\n",
    "            \n",
    "            G_2 = np.diag(y)\n",
    "            h_2 = self.C * np.ones((n, 1))\n",
    "            \n",
    "            G = cvxopt.matrix(np.vstack((G_1, G_2)))\n",
    "            h = cvxopt.matrix(np.vstack((h_1, h_2)))\n",
    "        \n",
    "        #A and b\n",
    "        A = cvxopt.matrix(y.reshape(1, -1))\n",
    "        b = cvxopt.matrix(np.ones((1, 1)))\n",
    "        \n",
    "        #We are now ready to solve the convex optimisation problem!\n",
    "        self.solution = cvxopt.solvers.qp(P, q, G, h, A, b)  \n",
    "        \n",
    "        #Save alphas\n",
    "        self.alphas = np.array(self.solution['x'])[:, 0]\n",
    "        \n",
    "        #Setup index lists!\n",
    "        support_alpha_indices = []\n",
    "        \n",
    "        for cnt, i in enumerate(self.alphas):\n",
    "            if i >= self.alpha_threshold:\n",
    "                if self.C is not None:\n",
    "                     if i != self.C: #SV = 0 < alpha_i < C\n",
    "                        support_alpha_indices.append(cnt)\n",
    "                \n",
    "                else:\n",
    "                    support_alpha_indices.append(cnt)\n",
    "                \n",
    "                        \n",
    "        \n",
    "        self.support_alpha_indices = support_alpha_indices\n",
    "        self.support_alphas = self.alphas[self.support_alpha_indices]\n",
    "        self.support_vectors = X[self.support_alpha_indices, :]\n",
    "        self.support_labels = y[self.support_alpha_indices]\n",
    "        self.sv_weights = self.support_alphas * self.support_labels #alpha_prime for sv's\n",
    "        \n",
    "        #Calculate center\n",
    "        self.center = np.sum(self.sv_weights.reshape(-1, 1) * self.support_vectors, axis = 0)\n",
    "        \n",
    "        #Calculate radius\n",
    "        sv_index = 0 #Use first support vector to compute radius\n",
    "        actual_index = self.support_alpha_indices[sv_index]\n",
    "        \n",
    "        #Compute sv_gram (will simply model evaluation and prediction)\n",
    "        self.Gram_svs = self.calculate_gram(self.support_vectors)\n",
    "        \n",
    "        t1 = Gram[actual_index, actual_index]\n",
    "        t2 = -2 * np.dot(self.sv_weights, self.Gram_svs[sv_index, :])\n",
    "        t3 = np.dot(self.sv_weights.reshape(-1, 1).T, np.dot(self.Gram_svs, self.sv_weights.reshape(-1, 1)))\n",
    "        \n",
    "        self.square_radius = t1 + t2 + t3\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_indices(N, start_fold, size_fold):\n",
    "        \n",
    "        idx_train = list(range(0, start_fold, 1)) + list(range(start_fold + size_fold, N, 1))\n",
    "        idx_test = list(range(start_fold, start_fold + size_fold, 1))\n",
    "        \n",
    "        return idx_train, idx_test\n",
    "    \n",
    "    def optimise_parameters(self, X, y = None, sigma_low = 1e-4, C_opt = False, N_iter = 50, loocv_flag = True, k_fold = None, plot_flag = False):\n",
    "        \n",
    "        N = X.shape[0]\n",
    "        \n",
    "        if y is None:\n",
    "            y = np.ones(N)\n",
    "            \n",
    "        else:\n",
    "            assert len(y) == N, \"The labels are not the same shape as the input data X.\"\n",
    "        \n",
    "        #Check that the user hasn't broken anything\n",
    "        if not loocv_flag: #k-fold loop\n",
    "            \n",
    "            assert k_fold is not None, \"k_fold variable should be an integer, not {}.\".format(k_fold)\n",
    "            size_fold = N // k_fold\n",
    "            \n",
    "        #Calculate the maximum sigma value that can be obtained\n",
    "        max_sigma = self.calculate_max_dist(X)\n",
    "        \n",
    "        #Define the sigma range\n",
    "        sigma_range = np.linspace(sigma_low, max_sigma, N_iter)\n",
    "        \n",
    "        #Set up the grid search points\n",
    "        if C_opt:#1/N leq C leq 1\n",
    "            if not loocv_flag:\n",
    "                C_range = np.linspace(1/(N - size_fold), 1, N_iter)  #Adjust because of fold size\n",
    "            \n",
    "            else:\n",
    "                C_range = np.linspace(1/(N), 1, N_iter)\n",
    "        else:\n",
    "            C_range = np.array([self.C])\n",
    "            \n",
    "        iter_list = list(itertools.product(C_range, sigma_range))\n",
    "        \n",
    "        #Define the loss array\n",
    "        loss_array = np.zeros(len(iter_list))\n",
    "        \n",
    "        #Save the original parameters\n",
    "        self.orig_params = copy.deepcopy(self.get_params())\n",
    "        \n",
    "        for cnt, iter_vals in enumerate(iter_list):\n",
    "            \n",
    "            print(\"\\nBeginning optimisation iteration {}...\".format(cnt + 1))\n",
    "            \n",
    "            #Get parameters\n",
    "            self.C = iter_vals[0]\n",
    "            self.sigma = iter_vals[1]\n",
    "            \n",
    "            if loocv_flag: #Simple loocv loop\n",
    "                \n",
    "                #fit\n",
    "                self.fit(X, y)\n",
    "                \n",
    "                #score\n",
    "                loss_array[cnt], _, _ = self.score(X, y, error_loocv = True)\n",
    "            \n",
    "            else: #k-fold loop\n",
    "                \n",
    "                #Get parameters\n",
    "                self.C = iter_vals[0]\n",
    "                self.sigma = iter_vals[1]\n",
    "                \n",
    "                loss_k_fold = np.zeros(k_fold)\n",
    "                cnt_k_fold = 0\n",
    "                start_fold = 0\n",
    "                \n",
    "                for k_index in range(k_fold):\n",
    "                    \n",
    "                    print(\"\\nBeginning k-fold iteration {}...\".format(k_index + 1))\n",
    "                    \n",
    "                    train_indices, test_indices = self.create_indices(N, start_fold, size_fold)\n",
    "                    \n",
    "                    X_train, X_test, y_train, y_test = X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "                    \n",
    "                    plt.figure()\n",
    "                    plt.scatter(X_train[:, 0], X_train[:, 1], marker = \"x\", color = \"r\")\n",
    "                    plt.scatter(X_test[:, 0], X_test[:, 1], marker = \"d\", color = \"b\")\n",
    "                    plt.show()\n",
    "                    \n",
    "                    #fit\n",
    "                    self.fit(X_train, y_train)\n",
    "                    \n",
    "                    #score\n",
    "                    loss_k_fold[cnt_k_fold], _, _ = self.score(X_test, y_test, error_loocv = False)\n",
    "                    \n",
    "                    cnt_k_fold += 1\n",
    "                    start_fold += size_fold\n",
    "                \n",
    "                #Score\n",
    "                loss_array[cnt] = np.mean(loss_k_fold)  \n",
    "        \n",
    "        self.opt_losses = loss_array\n",
    "        \n",
    "        opt_index = np.argmin(self.opt_losses)\n",
    "        \n",
    "        opt_dict = {\"C\":iter_list[opt_index][0], \"sigma\":iter_list[opt_index][1]}\n",
    "        \n",
    "        self.opt_params = opt_dict\n",
    "        \n",
    "        #Visualise\n",
    "        if plot_flag:\n",
    "            if C_opt:\n",
    "                x_vis, y_vis = np.meshgrid(C_range, sigma_range)\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.contourf(x_vis, y_vis, self.opt_losses.reshape(N_iter, N_iter))\n",
    "                plt.xlabel(\"C\")\n",
    "                plt.ylabel(r\"$\\sigma$\")\n",
    "                plt.show()\n",
    "                \n",
    "            else:\n",
    "                plt.figure()\n",
    "                plt.plot(sigma_range, self.opt_losses)\n",
    "                plt.xlabel(r\"$\\sigma$\")\n",
    "                plt.ylabel(\"Objective\")\n",
    "                plt.show()\n",
    "        \n",
    "        return opt_dict #C, sigma\n",
    "    \n",
    "    def evaluate_model(self, x):\n",
    "        #X = (N,) array\n",
    "        \n",
    "        pred_val = 0\n",
    "        \n",
    "        t1_pred = self.apply_kernel_function(x, x)\n",
    "        \n",
    "        t2_pred = 0\n",
    "        \n",
    "        for cnt, i in enumerate(self.sv_weights):\n",
    "            t2_pred += i * self.apply_kernel_function(x, self.support_vectors[cnt, :])\n",
    "        \n",
    "        t2_pred *= -2\n",
    "        \n",
    "        t3_pred = np.dot(self.sv_weights.reshape(-1, 1).T, np.dot(self.Gram_svs, self.sv_weights.reshape(-1, 1)))\n",
    "        \n",
    "        return t1_pred + t2_pred + t3_pred\n",
    "    \n",
    "    def predict(self, X, y = None):\n",
    "\n",
    "        if len(X.shape) > 1:\n",
    "\n",
    "            n = X.shape[0]\n",
    "            prediction = np.zeros(n)\n",
    "\n",
    "            for i in range(n):\n",
    "                prediction[i] = self.square_radius - self.evaluate_model(X[i, :])\n",
    "\n",
    "        else:\n",
    "            prediction = self.square_radius - self.evaluate_model(X)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def score(self, X, y = None, error_loocv = True):\n",
    "        N = X.shape[0]\n",
    "        \n",
    "        pred = self.predict(X, y)\n",
    "        \n",
    "        if y is not None:\n",
    "            pred *= y #Adjust for correctly labelled data\n",
    "        \n",
    "        #Calculate the error:\n",
    "        error1 = np.mean(pred < -1 * self.radius_threshold) #misclassification error\n",
    "        error2 = len(self.support_alpha_indices) / N #Loocv error\n",
    "        \n",
    "        if error_loocv: \n",
    "            cost = np.sqrt(error2**2 + np.abs(1 - np.sqrt(self.square_radius))**2)\n",
    "        \n",
    "        else:\n",
    "            cost = np.sqrt(error1**2 + np.abs(1 - np.sqrt(self.square_radius))**2)\n",
    "        \n",
    "        return cost, error1, error2\n",
    "    \n",
    "    def get_params(self):\n",
    "        param_dict = {\"C\":self.C, \"sigma\":self.sigma}\n",
    "        \n",
    "        return param_dict\n",
    "    \n",
    "    def set_params(self, param_dict):\n",
    "        self.C = param_dict[\"C\"]\n",
    "        self.sigma = param_dict[\"sigma\"]\n",
    "        \n",
    "        return self\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487821db",
   "metadata": {},
   "source": [
    "## Generate dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e1e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_sample = 1000\n",
    "R = 4\n",
    "x_data = np.random.random(N_sample) * 8 - 4\n",
    "y_data = np.random.random(N_sample) * 8 - 4\n",
    "\n",
    "X_data = np.vstack((x_data, y_data)).T\n",
    "\n",
    "R_data = np.sum(X_data**2, axis = 1)\n",
    "\n",
    "labels = 2 * (R_data < R**2) - 1\n",
    "\n",
    "x_plot = np.linspace(-4, 4, 400)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_data[:, 0], X_data[:, 1], c = labels)\n",
    "plt.plot(x_plot, np.sqrt(16 - x_plot**2), \"r--\")\n",
    "plt.plot(x_plot, -np.sqrt(16 - x_plot**2), \"r--\")\n",
    "plt.show()\n",
    "\n",
    "#Get data only in sphere\n",
    "X = X_data[np.nonzero(R_data < R**2)[0], :]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Data\")\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.plot(x_plot, np.sqrt(16 - x_plot**2), \"r--\")\n",
    "plt.plot(x_plot, -np.sqrt(16 - x_plot**2), \"r--\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba52e89",
   "metadata": {},
   "source": [
    "\n",
    "If we test new data, we can do so using the radius formula\n",
    "\\begin{equation}\n",
    "R^2 = k(\\mathbf{x}_k, \\mathbf{x}_k) - 2 \\sum_i a^{'}_i k(\\mathbf{x}_i, \\mathbf{x}_k) + \\sum_{i}\\sum_{j} k(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{x}_k$ is any support vector. This radius is used to test new data using:\n",
    "\\begin{equation}\n",
    "\\Vert \\mathbf{z} - \\boldsymbol\\alpha \\Vert_2^2 = k(\\mathbf{z}, \\mathbf{z}) - 2 \\sum_i a^{'}_i k(\\mathbf{z}, \\mathbf{x}_k) + \\sum_{i}\\sum_{j} k(\\mathbf{x}_i, \\mathbf{x}_j) \\leq R^2\n",
    "\\end{equation}\n",
    "\n",
    "we can subtract $R^2 - \\Vert \\mathbf{z} - \\boldsymbol\\alpha \\Vert_2^2$ to produce a positive value if a test sample is in the decision boundary, or a negative value otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f7c74",
   "metadata": {},
   "source": [
    "## Train model on dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svdd_test = SVDD('linear', C = 1)\n",
    "svdd_test.fit(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Alphas - dataset 1\")\n",
    "plt.scatter(np.arange(len(svdd_test.alphas)), svdd_test.alphas)\n",
    "plt.show()\n",
    "\n",
    "print(svdd_test.center, svdd_test.square_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5906b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svdd_test.score(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97e151",
   "metadata": {},
   "source": [
    "### Visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_grid = 100\n",
    "X_m, Y_m = np.meshgrid(np.linspace(np.min(X[:, 0]) - 2, np.max(X[:, 0]) + 2, N_grid), np.linspace(np.min(X[:, 1]) - 2, np.max(X[:, 1]) + 2, N_grid))\n",
    "\n",
    "X_grid = np.hstack((X_m.reshape(-1, 1), Y_m.reshape(-1, 1)))\n",
    "\n",
    "Z = svdd_test.predict(X_grid)\n",
    "\n",
    "plt.figure()\n",
    "plt.contour(X_m, Y_m, Z.reshape(N_grid, N_grid))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_test.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(r\"Contours - y($\\mathbf{x}$)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(X_m, Y_m, np.sign(Z.reshape(N_grid, N_grid)))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_test.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(\"On/Off visualisation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd12fb4",
   "metadata": {},
   "source": [
    "## Generate dataset 2 - including positive and negative labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc2bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.randn(100, 2) + np.array([4, 4])\n",
    "X2 = np.random.randn(100, 2) + np.array([-4, -4])\n",
    "X3 = np.random.randn(100, 2) + np.array([-3, 3])\n",
    "X4 = np.random.randn(100, 2) + np.array([3, -3])\n",
    "\n",
    "l1 = np.ones(100)\n",
    "l2 = np.ones(100)\n",
    "X = np.vstack((X1, X2, X3, X4))\n",
    "labels = np.hstack((l1, l2, -l1, -l2))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Data\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c = labels)\n",
    "plt.xlabel(r\"$x_1$\", fontsize = 14)\n",
    "plt.ylabel(r\"$x_2$\", fontsize = 14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b108691c",
   "metadata": {},
   "source": [
    "### Train model on dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48792beb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svdd_test = SVDD('gaussian', C = 0.8, sigma = 1)\n",
    "#opt_params = svdd_test.optimise_parameters(X, y = labels, sigma_low = 1e-4, C_opt = True, N_iter = 10, plot_flag = True)\n",
    "\n",
    "#print(opt_params)\n",
    "#set params\n",
    "#svdd_test.set_params(opt_params)\n",
    "\n",
    "#Fit model\n",
    "svdd_test.fit(X, labels)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(np.arange(len(svdd_test.alphas)), svdd_test.alphas * labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07d94f",
   "metadata": {},
   "source": [
    "### Visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c229c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_grid = 100\n",
    "X_m, Y_m = np.meshgrid(np.linspace(np.min(X[:, 0]) - 2, np.max(X[:, 0]) + 2, N_grid), np.linspace(np.min(X[:, 1]) - 2, np.max(X[:, 1]) + 2, N_grid))\n",
    "\n",
    "X_grid = np.hstack((X_m.reshape(-1, 1), Y_m.reshape(-1, 1)))\n",
    "\n",
    "Z = svdd_test.predict(X_grid)\n",
    "\n",
    "plt.figure()\n",
    "plt.contour(X_m, Y_m, Z.reshape(N_grid, N_grid))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_test.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(r\"Contours - y($\\mathbf{x}$)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(X_m, Y_m, np.sign(Z.reshape(N_grid, N_grid)))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_test.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(\"On/Off visualisation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ca070",
   "metadata": {},
   "outputs": [],
   "source": [
    "svdd_test.score(X, labels), svdd_test.square_radius"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb61074",
   "metadata": {},
   "source": [
    "## Generate dataset 3 - For optimising hyper-parameters without labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f465a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_data = 400\n",
    "X1 = np.random.randn(N_data, 2) + np.array([4, 4])\n",
    "X2 = np.random.randn(N_data, 2) + np.array([-4, -4])\n",
    "percentage = 0.8\n",
    "\n",
    "l1 = np.ones(N_data)\n",
    "l2 = np.ones(N_data)\n",
    "X = np.vstack((X1, X2))\n",
    "labels = np.hstack((l1, l2))\n",
    "\n",
    "range_all = np.arange(X.shape[0])\n",
    "np.random.shuffle(range_all)\n",
    "\n",
    "X = X[range_all]\n",
    "\n",
    "X_train = X[:int(percentage * X.shape[0]), :] \n",
    "labels_train = labels[:int(percentage * X.shape[0])]\n",
    "\n",
    "X_test = X[int(percentage * X.shape[0]):, :] \n",
    "labels_test = labels[int(percentage * X.shape[0]):]\n",
    "\n",
    "#X_test = np.vstack((X1, X2, X3, X4))\n",
    "#labels_test = np.hstack((l1, l2, -1 *  l1, -1 *  l2))\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Data\")\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c = labels_train)\n",
    "plt.xlabel(r\"$x_1$\", fontsize = 14)\n",
    "plt.ylabel(r\"$x_2$\", fontsize = 14)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Data - testing\")\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c = labels_test)\n",
    "plt.xlabel(r\"$x_1$\", fontsize = 14)\n",
    "plt.ylabel(r\"$x_2$\", fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a09d92",
   "metadata": {},
   "source": [
    "## Varying sigma and plotting the objective function from Theissler and Dear, 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4497f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_CV = 50\n",
    "cost1_sigma = np.zeros(N_CV)\n",
    "cost2_sigma = np.zeros(N_CV)\n",
    "error1_sigma = np.zeros(N_CV)\n",
    "error2_sigma = np.zeros(N_CV)\n",
    "radius_sigma = np.zeros(N_CV)\n",
    "set_C = 1\n",
    "\n",
    "#sigma variation\n",
    "sigma_array = np.linspace(1e-4, 25, N_CV)\n",
    "\n",
    "for cnt, sigma_cross in enumerate(sigma_array):\n",
    "    \n",
    "    svdd_test = SVDD('gaussian', C = set_C, sigma = sigma_cross)\n",
    "    svdd_test.fit(X_train)\n",
    "\n",
    "    cost1_sigma[cnt], error1_sigma[cnt], error2_sigma[cnt] = svdd_test.score(X_test, error_loocv = True)\n",
    "    cost2_sigma[cnt], _, _ = svdd_test.score(X_test, error_loocv = False)\n",
    "    \n",
    "    radius_sigma[cnt] = np.sqrt(svdd_test.square_radius[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e68929",
   "metadata": {},
   "source": [
    "### Visualise response for optimal sigma model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caedbfab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Loss - Sigma\")\n",
    "plt.semilogx(sigma_array, cost1_sigma, label = \"Theissler and Dear (2013) loss (loocv for Ew)\")\n",
    "plt.semilogx(sigma_array, cost2_sigma, label = \"Theissler and Dear (2013) loss (data for Ew)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Errors - Sigma\")\n",
    "plt.semilogx(sigma_array, error1_sigma, label = \"Error - trained\")\n",
    "plt.semilogx(sigma_array, error2_sigma, label = \"SV error (approximation)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Radius - Sigma\")\n",
    "plt.semilogx(sigma_array, radius_sigma, label = \"Radius\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\"What we can see here is that you NEED cross validation if you try to estimate the best fit from all the data\")\n",
    "print(\"Clearly, you overfit heavily for the case where you work out the error based on the data, because for small sigmas all the points become SVs\")\n",
    "\n",
    "svdd_sigma = SVDD('gaussian', C = 10, sigma = sigma_array[np.argmin(cost1_sigma)])\n",
    "svdd_sigma.fit(X)\n",
    "\n",
    "N_grid = 100\n",
    "X_m, Y_m = np.meshgrid(np.linspace(np.min(X[:, 0]) - 2, np.max(X[:, 0]) + 2, N_grid), \n",
    "                       np.linspace(np.min(X[:, 1]) - 2, np.max(X[:, 1]) + 2, N_grid))\n",
    "\n",
    "X_grid = np.hstack((X_m.reshape(-1, 1), Y_m.reshape(-1, 1)))\n",
    "\n",
    "Z = svdd_sigma.predict(X_grid)\n",
    "\n",
    "plt.figure()\n",
    "plt.contour(X_m, Y_m, Z.reshape(N_grid, N_grid))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_sigma.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(r\"Contours - y($\\mathbf{x}$)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(X_m, Y_m, np.sign(Z.reshape(N_grid, N_grid)))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_sigma.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(\"On/Off visualisation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeba4e4",
   "metadata": {},
   "source": [
    "## Varying the C parameter and plotting the objective function from Theissler and Dear, 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59d11b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_CV = 50\n",
    "cost1_C = np.zeros(N_CV)\n",
    "cost2_C = np.zeros(N_CV)\n",
    "error1_C = np.zeros(N_CV)\n",
    "error2_C = np.zeros(N_CV)\n",
    "set_sigma = sigma_array[np.argmin(cost1_sigma)] #Use previous solution\n",
    "radius_C = np.zeros(N_CV)\n",
    "\n",
    "#sigma variation\n",
    "C_array = np.linspace(1/X_train.shape[0], 1, N_CV)\n",
    "\n",
    "for cnt, C_cross in enumerate(C_array):\n",
    "    svdd_test = SVDD('gaussian', C = C_cross, sigma = set_sigma)\n",
    "    svdd_test.fit(X)\n",
    "    \n",
    "    cost1_C[cnt], error1_C[cnt], error2_C[cnt] = svdd_test.score(X_test, error_loocv = True)\n",
    "    cost2_C[cnt], _, _ = svdd_test.score(X_test, error_loocv = False)\n",
    "    \n",
    "    radius_C[cnt] = np.sqrt(svdd_test.square_radius[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b42ed",
   "metadata": {},
   "source": [
    "### Visualise response for optimal C model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d04c47f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Loss - C\")\n",
    "plt.plot(C_array, cost1_C, label = \"Theissler and Dear (2013) loss (loocv for Ew)\")\n",
    "plt.plot(C_array, cost2_C, label = \"Theissler and Dear (2013) loss (data for Ew)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Errors - C\")\n",
    "plt.plot(C_array, error1_C, label = \"SV error (approximation)\")\n",
    "plt.plot(C_array, error2_C, label = \"Error - trained\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Radius - C\")\n",
    "plt.plot(C_array, radius_C, label = \"Radius\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\"Same analysis here.\")\n",
    "\n",
    "svdd_C = SVDD('gaussian', C = C_array[np.argmin(cost1_C)], sigma = sigma_array[np.argmin(cost1_sigma)])\n",
    "svdd_C.fit(X)\n",
    "\n",
    "N_grid = 100\n",
    "X_m, Y_m = np.meshgrid(np.linspace(np.min(X[:, 0]) - 2, np.max(X[:, 0]) + 2, N_grid), \n",
    "                       np.linspace(np.min(X[:, 1]) - 2, np.max(X[:, 1]) + 2, N_grid))\n",
    "\n",
    "X_grid = np.hstack((X_m.reshape(-1, 1), Y_m.reshape(-1, 1)))\n",
    "\n",
    "Z = svdd_C.predict(X_grid)\n",
    "\n",
    "plt.figure()\n",
    "plt.contour(X_m, Y_m, Z.reshape(N_grid, N_grid))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_C.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(r\"Contours - y($\\mathbf{x}$)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(X_m, Y_m, np.sign(Z.reshape(N_grid, N_grid)))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_C.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(\"On/Off visualisation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739b8a2",
   "metadata": {},
   "source": [
    "## Summary of varying sigma and C parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c857da7",
   "metadata": {},
   "source": [
    "What I found after this investigation is that the LOOCV approach to calculating $E_w$ ($E_w = \\frac{\\#SV}{N}$) is a pretty good way of calculating the loss from Theissler and Dear (2013). I summarise my main conclusions here:\n",
    "\n",
    "- There does not appear to be a big improvement between using the LOOCV approximation to $E_w$ and the exact approach.\n",
    "- What was of CRUCIAL importance for the Theissler approach is that one uses a test set that is not like the test set to optimise the model. The reason for this is that at low $\\sigma$, all SVs are used to define the decision boundary. Hence, if you use the same data for training and testing, $E_w$ will be low but the model is poor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df1044d",
   "metadata": {},
   "source": [
    "## Optimising the hyper-parameters - Using the LOOCV error solution (thus not needing to do cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a7ba6",
   "metadata": {},
   "source": [
    "If loocv_flag = True, then there is no k-fold cross-validation. The SVDD object is designed to perform k-fold cross validation internally, if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svdd_loocv = SVDD('gaussian', C = 0.1, sigma = 1)\n",
    "opt_params = svdd_loocv.optimise_parameters(X, \n",
    "                                           y = None, \n",
    "                                           sigma_low = 1e-4,\n",
    "                                           C_opt = True, \n",
    "                                           N_iter = 10, \n",
    "                                           loocv_flag = True,\n",
    "                                           k_fold = None,\n",
    "                                           plot_flag = True)\n",
    "\n",
    "print(opt_params)\n",
    "\n",
    "#set params\n",
    "svdd_loocv.set_params(opt_params)\n",
    "\n",
    "#Fit model to all the data\n",
    "svdd_loocv.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7a6916",
   "metadata": {},
   "source": [
    "### Visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e4fc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_grid = 100\n",
    "X_m, Y_m = np.meshgrid(np.linspace(np.min(X[:, 0]) - 2, np.max(X[:, 0]) + 2, N_grid), \n",
    "                       np.linspace(np.min(X[:, 1]) - 2, np.max(X[:, 1]) + 2, N_grid))\n",
    "\n",
    "X_grid = np.hstack((X_m.reshape(-1, 1), Y_m.reshape(-1, 1)))\n",
    "\n",
    "Z = svdd_loocv.predict(X_grid)\n",
    "\n",
    "plt.figure()\n",
    "plt.contour(X_m, Y_m, Z.reshape(N_grid, N_grid))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_loocv.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(r\"Contours - y($\\mathbf{x}$)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(X_m, Y_m, np.sign(Z.reshape(N_grid, N_grid)))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_loocv.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(\"On/Off visualisation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655af783",
   "metadata": {},
   "source": [
    "## Optimising the hyper-parameters - Using the k-fold solution (thus performing cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "svdd_kfold = SVDD('gaussian', C = 0.1, sigma = 1)\n",
    "\n",
    "opt_params = svdd_kfold.optimise_parameters(X, \n",
    "                                           y = None, \n",
    "                                           sigma_low = 1e-4,\n",
    "                                           C_opt = True, \n",
    "                                           N_iter = 10, \n",
    "                                           loocv_flag = False,\n",
    "                                           k_fold = 5,\n",
    "                                           plot_flag = True)\n",
    "\n",
    "print(opt_params)\n",
    "\n",
    "#set params\n",
    "svdd_kfold.set_params(opt_params)\n",
    "\n",
    "#Fit model to all the data\n",
    "svdd_kfold.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e741d",
   "metadata": {},
   "source": [
    "### Visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ee272",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_grid = 100\n",
    "X_m, Y_m = np.meshgrid(np.linspace(np.min(X[:, 0]) - 2, np.max(X[:, 0]) + 2, N_grid), \n",
    "                       np.linspace(np.min(X[:, 1]) - 2, np.max(X[:, 1]) + 2, N_grid))\n",
    "\n",
    "X_grid = np.hstack((X_m.reshape(-1, 1), Y_m.reshape(-1, 1)))\n",
    "\n",
    "Z = svdd_kfold.predict(X_grid)\n",
    "\n",
    "plt.figure()\n",
    "plt.contour(X_m, Y_m, Z.reshape(N_grid, N_grid))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_kfold.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(r\"Contours - y($\\mathbf{x}$)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(X_m, Y_m, np.sign(Z.reshape(N_grid, N_grid)))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_kfold.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(\"On/Off visualisation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7093c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svdd_loocv.opt_losses, \"\\n\", svdd_kfold.opt_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fc56d6",
   "metadata": {},
   "source": [
    "## Comments at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4f8dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "svdd_loocv.opt_params, \"\\n\", svdd_kfold.opt_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74667098",
   "metadata": {},
   "source": [
    "Interestingly, for this problem, the optimal dictionaries for the k-fold and the LOOCV optimal solution are similar, and the optimal values are very close (only the C value is different, but this is expected as 1/N is different for k-fold (subtract the test set length). Finally, lets run the same code but with labels, just to be sure everything works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca314bf",
   "metadata": {},
   "source": [
    "## Dataset 4 - optimisation with labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81df45a3",
   "metadata": {},
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d8fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.randn(100, 2) + np.array([4, 4])\n",
    "X2 = np.random.randn(100, 2) + np.array([-4, -4])\n",
    "X3 = np.random.randn(100, 2) + np.array([-3, 3])\n",
    "X4 = np.random.randn(100, 2) + np.array([3, -3])\n",
    "\n",
    "l1 = np.ones(100)\n",
    "l2 = np.ones(100)\n",
    "X = np.vstack((X1, X2, X3, X4))\n",
    "labels = np.hstack((l1, l2, -l1, -l2))\n",
    "\n",
    "reorder = np.arange(X.shape[0])\n",
    "np.random.shuffle(reorder)\n",
    "\n",
    "X = X[reorder, :]\n",
    "labels = labels[reorder]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Data\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c = labels)\n",
    "plt.xlabel(r\"$x_1$\", fontsize = 14)\n",
    "plt.ylabel(r\"$x_2$\", fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6328f5ce",
   "metadata": {},
   "source": [
    "### LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b917848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svdd_loocv = SVDD('gaussian', C = 0.1, sigma = 1)\n",
    "opt_params = svdd_loocv.optimise_parameters(X, \n",
    "                                           y = labels, \n",
    "                                           sigma_low = 1e-4,\n",
    "                                           C_opt = True, \n",
    "                                           N_iter = 10, \n",
    "                                           loocv_flag = True,\n",
    "                                           k_fold = None,\n",
    "                                           plot_flag = True)\n",
    "\n",
    "print(opt_params)\n",
    "\n",
    "#set params\n",
    "svdd_loocv.set_params(opt_params)\n",
    "\n",
    "#Fit model to all the data\n",
    "svdd_loocv.fit(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_grid = 100\n",
    "X_m, Y_m = np.meshgrid(np.linspace(np.min(X[:, 0]) - 2, np.max(X[:, 0]) + 2, N_grid), \n",
    "                       np.linspace(np.min(X[:, 1]) - 2, np.max(X[:, 1]) + 2, N_grid))\n",
    "\n",
    "X_grid = np.hstack((X_m.reshape(-1, 1), Y_m.reshape(-1, 1)))\n",
    "\n",
    "Z = svdd_loocv.predict(X_grid)\n",
    "\n",
    "plt.figure()\n",
    "plt.contour(X_m, Y_m, Z.reshape(N_grid, N_grid))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_loocv.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(r\"Contours - y($\\mathbf{x}$)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(X_m, Y_m, np.sign(Z.reshape(N_grid, N_grid)))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_loocv.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(\"On/Off visualisation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c856d",
   "metadata": {},
   "source": [
    "### K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c935c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "svdd_kfold = SVDD('gaussian', C = 0.1, sigma = 1)\n",
    "\n",
    "opt_params = svdd_kfold.optimise_parameters(X, \n",
    "                                           y = labels, \n",
    "                                           sigma_low = 1e-4,\n",
    "                                           C_opt = True, \n",
    "                                           N_iter = 10, \n",
    "                                           loocv_flag = False,\n",
    "                                           k_fold = 5,\n",
    "                                           plot_flag = True)\n",
    "\n",
    "print(opt_params)\n",
    "\n",
    "#set params\n",
    "svdd_kfold.set_params(opt_params)\n",
    "\n",
    "#Fit model to all the data\n",
    "svdd_kfold.fit(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_grid = 100\n",
    "X_m, Y_m = np.meshgrid(np.linspace(np.min(X[:, 0]) - 2, np.max(X[:, 0]) + 2, N_grid), \n",
    "                       np.linspace(np.min(X[:, 1]) - 2, np.max(X[:, 1]) + 2, N_grid))\n",
    "\n",
    "X_grid = np.hstack((X_m.reshape(-1, 1), Y_m.reshape(-1, 1)))\n",
    "\n",
    "Z = svdd_kfold.predict(X_grid)\n",
    "\n",
    "plt.figure()\n",
    "plt.contour(X_m, Y_m, Z.reshape(N_grid, N_grid))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_kfold.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(r\"Contours - y($\\mathbf{x}$)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(X_m, Y_m, np.sign(Z.reshape(N_grid, N_grid)))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = svdd_kfold.predict(X), cmap = plt.cm.jet)\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()\n",
    "plt.title(\"On/Off visualisation\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
